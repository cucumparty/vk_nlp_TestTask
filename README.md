# Выравнивание языковой модели

## Описание        
Этот проект предназначен для дообучения языковой модели "IlyaGusev/saiga_llama3_8b" с использованием набора данных инструкций и ответов "OpenAssistant/oasst1". Модель дообучается отвечать в определенном стиле, например сарказм.

## Установка
1. Клонируйте репозиторий:
   ```bash
   git clone <repository-url>
   ```
2. Перейдите в директорию проекта:
   ```bash
   cd <project-directory>
   ```
3. Установите необходимые пакеты:
   ```bash
   pip install -r requirements.txt
   ```

## Использование
Чтобы дообучить модель и запустить процесс обучения, используйте следующую команду:

```bash
python alignment.py --mode=train --style=<style> --train_number=<train_num> --val_number=<val_num> --criteria_file=criteria.txt
```

Где:
- `--mode`: `train`, чтобы видеть ответы и оценки модели, иначе `test`
- `--style`: Указывает стиль ответа, на котором модель должна сосредоточиться (например, "сарказм").
- `--train_number`: Количество тренировочных образцов для использования.
- `--val_number`: Количество валидационных образцов для использования.
- `--criteria_file`: Путь к файлу, содержащему критерии оценки ответов (например, `criteria.txt`).

### Пример:
```bash
python alignment.py --mode=train --style=сарказм --train_number=100 --val_number=10 --criteria_file=criteria.txt 
```

## Файл критериев (`criteria.txt`)
Оценка ответов основана на файле критериев. Пример `criteria.txt` выглядит следующим образом:

```plaintext
1 — Совсем нет сарказма, ответ прямолинейный и серьезный. 
2 — Легкий намек на сарказм, но он неявен. 
3 — Сарказм заметен, но умеренный.
4 — Сарказм явный и достаточно ощутимый.
5 — Явный сарказм, ответ наполнен язвительными комментариями и насмешками.
```

Этот файл используется во время оценки для выставления оценок ответам модели и для управления обучением.

## Дообучение DPO  
В данном проекте мы применяем метод Direct Preference Optimization (DPO) для генерации ответов на инструкции. Сначала модель генерирует два варианта ответа для каждой инструкции, после чего эти ответы оцениваются этой же моделью по данным критериям. Лучший из двух ответов выбирается как предпочтительный и используется для дообучения модели, что позволяет улучшить качество генерации в дальнейшем.

## Оценка модели
После обучения модель генерирует два ответа для каждого входа в валидационном наборе:
- **Базовый ответ**: Начальный вывод от модели.
- **Ответ DPO**: Уточненный вывод после обучения DPO.

Результаты будут сохранены в файле `responses.json`.

Для сравнения результатов полученной модели относительно базовой на тестовой выборке использовалась ручная оценка.  
По результатам данной проверки получили, что после использования DPO, ответы начали выравниваться в нужную сторону.


